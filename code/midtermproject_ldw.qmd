---
title: "Mid-term project instructions"
author: "Jonathan S. CÃ¡rdenas"
format:
  html:
    code-fold: false
    embed-resources: true
    toc: true
    number-sections: true
    theme: journal
bibliography: references.bib
---

# Mid-term project GitHub repository URL:

https://github.com/JSCG95/2024dsa_midproject_group11.git

# Instructions

This file contains both the **instructions** for the mid-term project and placeholders for your code. You are required to use this file to produce code, output, and answers to the questions below.

Besides simply creating output, make sure to interpret the output. You will need to create tables and/or plots to arrive at the answers, and then comment on what you found below it.

To get you setup, you will need to:

-   Student #1: create a repository on your GitHub account. You can call this repository "2024dsa_midproject_groupX", **where X is the number of your group**. Make it public, add a README, add a .gitignore for R.\
-   Student #1: follow the steps we did in class to start a new RStudio project with version control.\
-   Student #1: in your computer, create the sub-folders code, data, output, and move your data set into the `data` folder. Also, student 1 moves this current script into the `code` folder. Do a git stage, commit, push.\
-   Student #1: on GitHub, go the repository settings and invite your partner to be a collaborator in the repository. That will give them push permission.\
-   Now, both students should clone this repository on their computers like we did in class. Make sure this step works well and that you can pull and push from GitHub.\
-   Student 2, after cloning, does a git pull to get all these updates on their computer.\
-   Student 1 and 2 work together to update the README file. README files should explain what the repository is about, the goals of that project, who is working in it, and any other important details you may find.

# Introduction

The convergence of remote sensing and computer vision promises to revolutionize plant research and crop monitoring by providing more efficient and insightful data collection methods within controlled environments. The true potential lies in the transformation of sensor-derived numerical representation of objects shapes or pixel intensities into meaningful parameters with biological relevance [@fahlgren2015].

Indoor horticulture benefits from a partial or complete environmental control that provides a consistent productivity. Destructive approaches limit a continuous evaluation of environmental effects on a highly dynamic plant response such as crop yield [@mochida2019].RGB sensors capturing color information from the red, green and blue bands is a highly versatile, ubiquitous and accessible tool as an image-based solution to add efficiency to the data collection process.

Lettuce canopy architecture could be described using two-dimensional (2D) top view RGB captures. Besides canopy projected area, additional shape properties provide a more comprehensive description of plant structure [@devylder2012]. In this study, an image analysis workflow was implemented to obtain geometrical features from multiple lettuce canopies simultaneously growing hydroponically. The same image-derived features were implemented as potential predictors for simple linear regress

Paragraph 4

# Hypothesis and objectives

HYPOTHESIS:

Romaine lettuce cultivar Dragoon has a more responsive cultivar to light treatments

Lettuce fresh weight per plant is accurately estimated using canopy geometric features extracted using an RGB image analysis and machine learning models.

-   Evaluate the effect of supplemental light on the leaf area of two lettuce cultivars.

-   Extract multiple geometric features for each plant canopy as potential predictors.

-   

# Material and Methods

Describe here your overall material and methods as it pertains to the analysis you will conduct, including study description, site/setup description, what equipment was used, etc. just like you would in a paper. Make sure to clearly explain what was measured and how.

## Study design

Clearly describe your study design here, including treatment design (which factors and levels, the hierarchy among them, etc.), and your experimental design (number of reps/blocks, how was randomization performed, etc.), as we talked about in class.

## Statistical analysis

Describe here your statistical analysis, including what type of ANOVA model you ran (based on the design above), what was your response variable, what were your explanatory variables and how were the explanatory variables treated (random or fixed). Provide your alpha level. Explain which function from which package you used to analyze this data. Explain how you checked linear model assumptions and whether or not they were met. Overall, make sure you explain in sufficient detail that, if given your data, a knowledgeable person would be able to reproduce your analysis exactly.

# Results

## Exploratory Data Analysis (EDA)

```{r setup}
#| message: false
#| warning: false

# Loading packages
library(tidyverse) # for data wrangling and plotting
library(janitor) # clean column names
library(knitr) # for figure displaying
library(car) # for Anova function
library(lme4)
library(broom) # for model residuals extraction
library(emmeans) # for model mean extraction
library(multcomp) # for pairwise comparison letter display
library(nlme) # for repeated measure correlation structures 
library(ggridges)
library(ggcorrplot)
library(nlraa)
library(caret)
```

Loading data tables including remote sensed data, treatment factor columns, response variables (leaf fresh weight, leaf dry weight and leaf area), harvest dates.

```{r}
#| message: false
#| warning: false
raw_data <- read_csv("../files/20-02_feat-matrix.csv")
wr_data <- raw_data %>%
  clean_names() %>% 
  dplyr::select(-c("tipburn",'percent_tipburn',
                   "cum_dli","cf")) %>%
  na.omit()
```

Transform treatments into factors and create a new column where each treatment is specified. Set date as date format to calculate DAT.

```{r}
tr_date  <- mdy('10/18/2023')

wr_data <-wr_data %>%
  mutate(date = mdy(date),
         tray_id = factor(tray_id),
         light = factor(light),
         dat = as.integer(date - tr_date),
         fdat = as.factor(dat),
         fcultivar = factor(cultivar)
         ) %>%
  mutate(trt_id = as.factor(paste0(cultivar, '+',light,'+',tray_id)),
         trt_name = as.factor(paste0(cultivar, '+',light))) %>%
  dplyr::select(-c(melatonin))
```

## Boxplots: Response Variables distribution.

```{r}
#| fig-width: 12
#| fig-height: 8
wr_data %>%
  ggplot(aes(x = trt_name,
           y = ldw_g,
           fill = light
           )
       ) +
  geom_boxplot(width = 0.4)+
  geom_jitter(alpha = 0.5,
              width = 0.3) +
  theme_bw()+
  facet_grid(dat~.,
             scales = 'free') +
  theme(plot.title = element_text(size=16,hjust = 0.5, face="bold"),
                   axis.text = element_text(size = 14,color="black"),
                   axis.title = element_text(size = 14, face="bold"),
                   legend.position = "top",
        legend.text = element_text(size = 14),  # Adjust legend text size
        legend.title = element_text(size = 14, face = 'bold'),
        panel.spacing = unit(.6, "lines"),
        panel.border = element_rect(color = "black", fill = NA, size = 1),
        strip.background = element_rect(fill = "gray", color = "black", size = 1),
    strip.text = element_text(size = 14, face = "bold", color = "white"))+
  labs(title=NULL,
       subtitle = NULL,
       caption=NULL,
       x=NULL,
       y="Leaf Fresh Weight (g)")
```

Means per tray as my replicate for the ANOVA analysis.

```{r}
mean_lfw <- wr_data %>%
  group_by(fcultivar,dat,fdat,light,trt_id, trt_name) %>%
  summarise(n = n(),
            mean = mean(ldw_g))
```

```{r}
mean_lfw %>%
  ggplot(aes(x = trt_name,
           y = mean,
           fill = light
           )
       ) +
  geom_boxplot(width = 0.4)+
  geom_jitter(alpha = 0.5,
              width = 0.3) +
  theme_bw()+
  facet_grid(dat~.,
             scales = 'free') +
  theme(plot.title = element_text(size=16,hjust = 0.5, face="bold"),
                   axis.text = element_text(size = 14,color="black"),
                   axis.title = element_text(size = 14, face="bold"),
                   legend.position = "top",
        legend.text = element_text(size = 14),  # Adjust legend text size
        legend.title = element_text(size = 14, face = 'bold'),
        panel.spacing = unit(.6, "lines"),
        panel.border = element_rect(color = "black", fill = NA, size = 1),
        strip.background = element_rect(fill = "gray", color = "black", size = 1),
    strip.text = element_text(size = 14, face = "bold", color = "white"))+
  labs(title=NULL,
       subtitle = NULL,
       caption=NULL,
       x=NULL,
       y="Leaf Fresh Weight (g)")
```

## 2-WAY ANOVA

```{r}
mod_default <- lme(
  mean ~ light*fcultivar*fdat,
  random = ~ fdat|trt_id,
  data = mean_lfw)


mod_arma2 <- lme(
  mean ~ light*fcultivar*fdat,
  random = ~ fdat|trt_id,
  correlation = corARMA(p = 1, q = 1),
  data = mean_lfw)


mod_AR1 <- lme(
  mean ~ light*fcultivar*fdat,
  random = ~fdat|trt_id,
  correlation = corCAR1(form = ~dat|trt_id),
  data = mean_lfw)


mod_gls <- gls(mean ~ light*fcultivar*fdat,
  correlation = corCAR1(form = ~dat|trt_id),
  data = mean_lfw)
```

```{r}
Anova(mod_gls, type = 3)
```

```{r}
ACF(mod_default, resType = "n") %>%
  plot(alpha = 0.01)

ACF(mod_AR1, resType = "n") %>%
  plot(alpha = 0.01)

ACF(mod_gls, resType = "n") %>%
  plot(alpha = 0.01)
```

```{r}
anova(mod_default,mod_arma2,mod_AR1, mod_gls)
```

## ANOVA assumptions

```{r}
library(broom.mixed)
mod_default_resid <- augment(mod_default) %>%
  mutate(.stdresid=resid(mod_default, type="pearson", scaled=T))

mod_default_resid <- augment(mod_default) %>%
  mutate(.stdresid=resid(mod_default, type="pearson", scaled=T))

mod_gls_resid <- augment(mod_gls) %>%
  mutate(.stdresid=resid(mod_gls, type="pearson", scaled=T))
```

```{r}
ggplot(mod_default_resid, aes(x=.fitted, y=.stdresid))+
  geom_hline(yintercept = 0, color="red")+
  geom_point(shape = 21,
             fill = "purple", 
             size = 3,
             alpha = .7)+
  geom_smooth()+
  geom_hline(yintercept = c(-3,3), color = "red")+
  theme_bw()
```

```{r}
Anova(mod_gls, type = 3)
```

```{r}
means_extract <- emmeans(mod_default, 
                         ~fcultivar:light|fdat) %>%
  cld(reversed = T,
      Letters = letters,
      adjust = "none"
      ) %>%
  as.data.frame()%>% 
  mutate(letter = trimws(.group)) %>%
  mutate(trtname = paste0(fcultivar,"+",light))

means_extract
```

## Emmeans graph

```{r}
#| fig-width: 6
#| fig-height: 8
ggplot(mapping = aes(fill = fcultivar))+
  # Raw data and boxplots  
  geom_boxplot(data = mean_lfw,
               aes(x = trt_name,
           y = mean,
           fill = fcultivar)) +
  geom_jitter(data = mean_lfw,
               aes(x = trt_name, 
                   y = mean),
              shape = 21,
              size = 2,
              alpha = .6) +
  # Adding letters
  geom_label(data = means_extract,
            aes(x = trtname, 
                y = emmean, 
                label = letter),
            fill = "white") +
  labs(x = "Treatments",
       y = "Leaf Fresh Weight (g)") +
  scale_fill_brewer(palette = "Accent") +
  facet_grid(fdat~.,
             scales = 'free_y') +
  theme(plot.subtitle = element_text(size=14, face="bold"),
                   axis.text.x = element_text(size=12),
                   axis.text.y = element_text(size=12),
                   axis.title.x = element_text(size=14,face="bold"),
                   axis.title.y = element_text(size=14,face="bold"),
                   axis.title = element_text(size = 14, face="bold"),
                   legend.position = "top",
        panel.spacing = unit(.5, "lines"),
        panel.border = element_rect(color = "black", fill = NA, size = 1),
        strip.background = element_rect(fill = "gray", color = "black", size = 1),
    strip.text = element_text(size = 12, face = "bold", color = "white"))
  
ggsave("../outputs/15-04_ldw-time.png",width = 8,height = 10,units = c("in"), dpi = 350)
```

## Response variables and predictors

To facilitate the analysis down this workflow, the original data set was divided into multiple data frames that contains all imaging features that will be used as potential predictors. Additionally, a data frame was created with all the response variables and predictors combined to implement an iterative step.

```{r}
predictors <- wr_data %>% dplyr::select(11:20) 

response <- wr_data %>% dplyr::select(c('ldw_g'))

matrix <- predictors %>%
  mutate(ldw_g = response$ldw_g) 

matrix_resp <- predictors %>%
  mutate(ldw_g = wr_data$ldw_g,
         ldw_g = wr_data$ldw_g,
         ldw_g = wr_data$ldw_g) 
 
```

## Baseline model: Best Predictor

To determine what is the best image parameter that best explains the variability of the response, the coefficient of determination - R^2^ was extracted from all possible linear regressions between 3 responses & 10 potential predictors. The image parameter showing the greatest value was used as the predictor for the baseline simple linear regression.

```{r}
r2_mods <- matrix_resp %>%
  # Pivotting longer for the response variables
  pivot_longer(cols = ldw_g:ldw_g,
               names_to = "resp_var",
               values_to = "resp_val"
               ) %>%
  # Pivotting longer for the explanatory variables  
  pivot_longer(cols = perimeter:inc_light,
               names_to = "exp_var",
               values_to = "exp_val"
               ) %>%
  # Creating groups of resp_var and exp_var
  group_by(resp_var,exp_var) %>%
  nest() %>%
  # Applying linear model to each element
  mutate(lm = map(data,
                  ~lm(resp_val ~ exp_val,
                      data = .x
                      )
                  )) %>%
  # Extracting R2 for each element  
  mutate(r2 = map(lm,
                  ~ summary(.x)$r.squared
                  )
         )

r2_mods
```

Only the features with the top 3 greatest R^2^ values are shown.

```{r}
top_r2 <- r2_mods %>%
  unnest(r2) %>%
  group_by(resp_var)  %>%
  arrange(desc(r2),resp_var) %>%
  slice(1:3) %>%
  mutate(r2 = round(r2,3))


```

A data frame with the top 3 features per variable created above was used to generate scatterplots showing the potential predictor on the x-axis and the biomass response on the y-axis (fresh weight, dry weight or leaf area).

```{r}
#| fig-width: 15
#| fig-height: 10

lm_plots <- top_r2 %>%
  mutate(splot = map2(data,r2,
                      ~ ggplot(data = .x,
                               aes(x = exp_val,
                                   y = resp_val)
                               ) +
                        geom_point()+
                        geom_smooth(method = 'lm') +
                        labs(title = paste("R\u00B2",.y))
                      
                      )
         ) %>%
  mutate(splot_better = pmap(
    list(.df = data, 
         .r2 = r2,
         .rv = resp_var,
         .ev = exp_var
         ),
    function(.df,.r2,.rv,.ev) 
      ggplot(data = .df,
                               aes(x = exp_val,
                                   y = resp_val)
                               ) +
                        geom_point(color = "black",
                                   shape = 21,
                                   size = 1.5, 
                                   stroke = 0.7,
                                   fill = 'red3') +
                        geom_smooth(method = 'lm', 
                                    color = "black") +
                        theme_bw() +
                        labs(title = paste(.ev,"\nR\u00B2:", .r2),
                             
                             x = .ev, 
                             y = .rv
                             )))

library(gridExtra)

linear_plots <- marrangeGrob(lm_plots$splot_better,
                         nrow = 3,
                         ncol = 3)

linear_plots
```

The following table summarizes the coefficient of determination (R^2^) values chosen for each response variable.

|     |        Leaf fresh weight         |          Leaf dry weight          |         Leaf area         |
|------------------|:----------------:|:----------------:|:----------------:|
| 1   | **incident light (R^2^= 0.784)** |      diameter (R^2^= 0.806 )      |  diameter (R^2^= 0.847 )  |
| 2   |      diameter (R^2^= 0.775)      | **incident light (R^2^= 0.801 )** |  area_cm2 (R^2^= 0.828 )  |
| 3   |      area_cm2 (R^2^= 0.770)      |      area_cm2 (R^2^= 0.793 )      | axis_minor (R^2^= 0.782 ) |

: Top 3 predictors with the largest coefficient of determination values for the 3 responses variables

## Assumptions for the best predictor

Incident light showed the greatest R^2^ to predict fresh biomass. This variable combines canopy area information with the amount of light received until a specific time point measured using a light sensor (cumulative DLI).

```{r}
options(contrasts = c("contr.sum", "contr.poly"))

# Model fitting: Because SR is numerical, will give us a slope. 
lfw_lin<- lm(ldw_g ~ inc_light,
                 data = matrix_resp)

summary(lfw_lin)

lfw_lin_aug <- augment(lfw_lin) %>%
  mutate(.stdresid = rstudent(lfw_lin))

ggplot(lfw_lin_aug, 
       aes(x = .fitted, 
           y = .stdresid))+
  geom_hline(yintercept = 0, color = "red")+
  geom_hline(yintercept = c(-3,3), color = "red")+
  geom_point(size = 3, alpha = .6)+
  geom_smooth()

```

## Linear or non - linear?

Destructive measurements and remote sensed data were captured at 3 time points along the crop cycle. As the objective is to fit a single model that predicts biomass on any given day, the right linear or non-linear fit between the best predictor must be found.

```{r}
options(contrasts = c("contr.sum", "contr.poly"))

# Model fitting: Because SR is numerical, will give us a slope. 
lfw_quad<- lm(ldw_g ~ inc_light + I(inc_light^2) ,
                 data = matrix_resp)
summary(lfw_quad)

lfw_quad_aug <- augment(lfw_quad) %>%
  mutate(.stdresid = rstudent(lfw_quad))

ggplot(lfw_quad_aug, 
       aes(x = .fitted, 
           y = .stdresid))+
  geom_hline(yintercept = 0, color = "red")+
  geom_hline(yintercept = c(-3,3), color = "red")+
  geom_point(size = 3, alpha = .6)+
  geom_smooth()

ggplot(data = matrix_resp, aes(x = inc_light, y = ldw_g)) +
geom_point() +
geom_line(aes(y = fitted(lfw_quad)))
```

```{r}
lfw_log = nls(ldw_g ~ SSlogis(inc_light, a, b,c), 
            data = matrix_resp)

summary(lfw_log)

lfw_log_aug <- augment(lfw_log) %>%
  mutate(.stdresid = resid(lfw_log, 
                           type = "pearson", 
                           scaled = T))

ggplot(lfw_log_aug, 
       aes(x = .fitted, 
           y = .stdresid))+
  geom_hline(yintercept = 0, color = "red")+
  geom_hline(yintercept = c(-3,3), color = "red")+
  geom_point(size = 3, alpha = .7)+
  geom_smooth()

ggplot(data = matrix_resp, aes(x = inc_light, y = ldw_g)) +
geom_point() +
geom_line(aes(y = fitted(lfw_log)))

```

Based on each model assumptions, a baseline model that fits

## Correlations among predictors

To assess a potential risk of multicollinearity among predictors, a correlation matrix based on Pearson's Correlation Coefficient (r) is plotted to check how linearly correlated the potential predictors are.

```{r}
#| fig-width: 12
#| fig-height: 8
corr_percent <- round(cor(predictors),2)
p.mat <- cor_pmat(predictors)

ggcorrplot(corr_percent, hc.order = TRUE, 
           ggtheme = theme_classic(),
           legend.title = "Pearson's (r) " ,
           outline.color = "black",
           type = "full",
           lab = TRUE,
           lab_size = 5,
           tl.cex = 14,
           tl.col = "black") +
    theme(legend.title = element_text(size=14, face = "bold"),
          legend.text = element_text(size=12))
```

The level of multicollinearity among 7 out of 10 predictors is high. To address this problem, a multiple linear regression fitted based on the variables that show the lowest variance inflation factor (VIF) when using simultaneously in a model will be used. Additionally, LASSO regression will be implemented as an alternative option, as this model penalizes predictors that do not contribute significantly.

## Modelling

### Multiple Linear Regression

Incident light was calculated based on canopy area, therefore adding both features together causes. Based on the R^2^ values and the biological significance of incident light, we will use incident light instead of area_cm2 and diameter, that essentially are giving us the same information.

```{r}
full_mlr<- lm(ldw_g ~ ., data  = matrix)
summary(full_mlr)

    
full_mlr %>%
  vif()
```

A full model alternative is summarized in the chunk below where many of the variable coefficients are not significant. Additionally, the VIFs for all the variables included are displayed to show how correlated most of the geometric variables are. Using the correlation matrix plotted in subsection 6.7 and based on the knowledge of what the morphometric variables described about the canopy shape, a reduced group of variables was selected.

```{r}
vif_mlr<- lm(ldw_g ~ inc_light + convex_hull + compactness  , data  = matrix) 

summary(vif_mlr)

lfw_vif_aug <- augment(vif_mlr) %>%
  mutate(.stdresid = resid(vif_mlr, 
                           type = "pearson", 
                           scaled = T))

ggplot(lfw_vif_aug, 
       aes(x = .fitted, 
           y = .std.resid))+
  geom_hline(yintercept = 0, color = "red")+
  geom_hline(yintercept = c(-3,3), color = "red")+
  geom_point(size = 3, alpha = .7)+
  geom_smooth()

vif_mlr %>%
  vif()
```

By keeping the least correlated predictors and selecting the variables that portrait heterogeneous information of that canopy shape, we obtained a MLR model with variables with highly significant coefficients.Additionally, the VIFs for the 3 variables selected are below 5.

## Predictions: Training and Testing.

### Data partition: Test-train split available

Until this point we found that to predict LFW, a quadratic model of incident light seems to be the best baseline model to be used for prediction as shown by it's high capacity for capturing variability in the response, a better description of the residuals variability as shown by the assumptions plot.

Here, I'm dropping the variables area_cm2 and diameter as I know ahead that incident light is capturing the same information being captured by these 2. As we will start using a machine learning approach to find the model that yields the most accurate prediction, our full dataset will be split in a training and a testing data set. Plant observation on the training set will be used to find the values of the hypertuning parameters for the models that require a hyperparameter specification (in our case: LASSO and random forest).

```{r}
matrix_alt <- matrix %>%
  dplyr::select(-c(diameter, area_cm2))

# Partition data and create index matrix of selected values
index <- createDataPartition(matrix_alt$ldw_g, 
                             p=.8, 
                             list=FALSE, 
                             times=1)

set.seed(20)
# Create test and train data frames
train_df <- matrix_alt[index,]
test_df <- matrix_alt[-index,]
```

### Model 3: LASSO Regression

Before training we need a resampling scheme. Here we are using a 10-fold cross validation

```{r}
# Specify 10-fold cross-validation as training method
cv_10 <- trainControl(method="cv", 
                          number=10,
                          savePredictions="all")
```

LASSO implements a penalty (lambda value) that is a threhold to reject variables with insignificant contribution to the model predictions. The following vector creates 500 different possible values for lambda. We want to find the valye of lambda that gives the minimum MSE.

```{r}
lambda_vector <- 10^seq(2, -5, length=500)
```

```{r}
# Set seed for reproducible random selection and assignment operations
set.seed(123) 

# Specify lasso regression model to be estimated using training data
# and k-fold cross-validation process
lfw_lasso <- train(ldw_g ~., 
                data=train_df,
                preProcess=c("center","scale"),
                method="glmnet", 
                tuneGrid=expand.grid(alpha=1, lambda=lambda_vector),
                trControl=cv_10,
                na.action=na.omit)
```

Showing the average performance for 2 performance metrics (R^2^ and RMSE).

```{r}
lamb_best<- lfw_lasso$bestTune$lambda


perf_lambda <- data.frame(lfw_lasso$results$Rsquared, 
                   lfw_lasso$results$lambda,
                   lfw_lasso$results$RMSE) %>%
  rename(R2 = lfw_lasso.results.Rsquared,
         lambda = lfw_lasso.results.lambda,
         RMSE = lfw_lasso.results.RMSE) %>%
  filter(lambda == lamb_best)


```

```{r}
coef(lfw_lasso$finalModel, lfw_lasso$bestTune$lambda)
```

```{r}
varImp(lfw_lasso)
```

### Random Forest

```{r}
rf_grid <- expand.grid(.mtry = c(2, 5,10))  # Number of variables to sample as candidates at each split

set.seed(123) 
# Fit the random forest model
lfw_rf <- train(ldw_g ~ .,
                data = train_df,
                method = "rf",
                trControl = cv_10,
                tuneGrid = rf_grid,
                na.action = na.omit)
```

```{r}
perf_rf <- data.frame(lfw_rf$results$Rsquared, 
                   lfw_rf$results$mtry,
                   lfw_rf$results$RMSE) %>%
  rename(R2 = lfw_rf.results.Rsquared,
         mtry = lfw_rf.results.mtry,
         RMSE = lfw_rf.results.RMSE) 

perf_rf
```

## Predictions performance using test set

Until this point we generated 3 predicting models: Baseline with incident_light quadratic, lasso regression and random forest regression. To run prediction with each model simultaneously I will iterate through the model's names.

```{r}
# Load the necessary library
library(purrr)

# Create a list of models
models <- list(lfw_quad, lfw_lasso,lfw_rf)

# Use map to apply the predict function to each model
predictions <- models %>%
  map(~ predict(., newdata = test_df)) %>%
  set_names(c("quadratic", "lasso", "random_forest")) %>%
  bind_cols()
```

With the next step, we will create a data frame that compares the performance for all models based on unseen information.

```{r}
calculate_performance <- function(predicted_values, actual_values) {
  data.frame(
    RMSE = caret::RMSE(predicted_values, actual_values),
    Rsquared = caret::R2(predicted_values, actual_values),
    MAPE = ie2misc::mape(predicted_values, actual_values)
  )
}
```

```{r}
performance_df <- map_dfr(names(predictions), ~{
  calculate_performance(predictions[[.x]], test_df$ldw_g) %>%
    mutate(names = .x) %>%
    arrange(desc(RMSE))
})

```

```{r}
scatter_test = map(names(predictions),
                      ~ ggplot(data = predictions,
                               aes(x = .data[[.x]],
                                   y = test_df$ldw_g)
                               ) + 
              geom_point(color = "black",shape = 21, 
                         size = 2, fill = "red3",
                         stroke = 1)+
              geom_smooth(method = 'lm',
                          color = "black") +
              theme_bw()+
              labs(title = paste(.x, 'predictions'), 
                   caption=NULL,x="Measured LFW (g)",
                   y="Predicted LFW (g)")) 
                      
scatter_plots <- marrangeGrob(scatter_test,
                         nrow = 3,
                         ncol = 1)

scatter_plots
```

Here is where the coding is going to happen, and it will be completely up to you. Include under this section as many sub-sections (using ##) and as many chunks needed to create the analytical workflow for your analysis, starting at loading packages and data, wrangling, EDA, modeling, assumptions checking, ANOVA table, means, pairwise comparisons, and final publication-quality plot.

Make sure to run a model that reflects your study design. Even if your study design does not include one of the designs covered in class, you are still expected to run the most appropriate model. If you need help for references, let me know.

Before each chunk, describe the steps being performed in that chunk. For example, "Here I will load the data".

If a chunk produces output, like printing a data frame, statistical summary, a plot, ANOVA table, etc., make sure to write text interpreting what you see and how you can/will use that information to move forward to the next steps in the workflow.

# Team work in GitHub

Whether you are working with your future-self or as duos, make sure to stage, commit, and push after finishing each of the sub-sections above. When committing, write commit messages that are short and descriptive (e.g., finished wrangling).

If you are working in duos, make sure to split the workload. I will check your collaborative work through the commit history, and if one student has contributed significantly more than the other, than that will impact grades.

**Tip 1**: to avoid merge conflicts, make sure to **pull** first, and then start working locally. That will ensure that any changes made by your partner will be "downloaded" before you make changes to the files locally.

**Tip 2**: make use of the Issues on this repository to set up to-do lists and assign tasks to different people. You can also use each issue/task to discuss how things should be run and get to an agreement.

# Submitting your work

Once you have developed all the code and answers, make sure to Render this quarto file.

**Notes on rendering**:

-   Make sure to render your work and inspect how the final html look like.\
-   If it does not look professional for whatever reason, then fix the issue, re-render it, recheck.\
-   Only send me your work once your html file looks professional.\
-   Some potential issues you may encounter and how to fix them:
    -   Some times your code may be creating a table output that is waaay to long and cumbersome to scroll through when rendered. If this is the case, make it more professional looking by using the `head()` function to only print the first handful of rows (instead of thousands of rows).

    -   **DO NOT** delete the file's heading levels (# and ##). They set up the proper heading 1 and 2 levels, and I use them to guide my grading.

    -   If a given chunk is also outputting warnings or messages, inhibit this behavior by changing the chunk options `message` and `warning` to `FALSE`.

    -   If, after rendered, 2 lines of text are connected and you wish to "break line" between them, add 2 extra spaces after the first one.

After rendering, an .html file will be created on your `code` folder.

Rename this file to `LASTNAME1-LASTNAME2_midtermproject.html`.\
For ex., `Bastos-Mendes_midtermproject.html`.

Send the html file to my email (lmbastos\@uga.edu) by **April 11th** 11:59 pm.
